- 21:01 start as incident communicator.
	-
- Status:
	- Doesn't seem to be a problem in DE and FR, only NL.
-
- Impact , 100% customer requests fail; nothing works.
-
- Update 21:10
	- Impact: 100% of customer requests are failing in NL.
	- DE and FR not impacted.
-
	- Update RCA:
		- We are currently disabling various pages with complex Calcite queries
			- :check: Basked recommendations disabled
			- :check: A2D home page disabled
			- :check: Previously bought
			- :check: Category activation campaigns
			- :check: Altijd handig
			- :question: Jouw acties
			-
		- Scaled horizontally, impact not fully clear yet.
-
- Update 21:25
	- No progress on RCA yet.
		- Observation: queued amount of queries (85.000+) is so high it should either be a system or a query that potentially has a loop.
	-
	- **Current attempt to mitigate the problem: triggered a restart with x4 CPU.**
		- One of the reasons: DataDog data is not updating.
		- Resource usage so high we cannot get healhty.
	-
	- Idea is to get a banner live on the website with "important announcement".
		- Because questions from customers are coming in.
		- Weren't able to reach someone yet that can put a banner on the website yet.
		- Currently calling the webbies.
	-
	- Current open questions:
		- Could it be that we are being DDoS'ed?
		- Which queries are being executed most often?
			- Variable MOV, p90 latency of 5 seconds.
		- Can we stop traffic to Store for a bit?
		-
	-
- **Update 21:30**
	- Restarted with x4 CPU's, **now pages are slowly coming back.**
	- Error Rate going from 95% -> 1%.
		-
		-
	- **Current hypothesis:** insane amount of request, running a script or DDoSing is still an possibility.
-
- **Update 21:40:**
- **App is back up and stable**; old pods terminated and only the ones with high CPU are available.
- Unclear what is causing this increased resource usage.
-
- Unlikely that this load is possible by "normal app usage".
-
- For new pods behavior seems to be normal now
-
- **Update 21:48**
- **Decision**: we are going to slowly start enabling the following elements (with intervals of 5 minutes):
	- Previously bought ("Opnieuw bestellen")
	- Your promotions ("Jouw acties")
	- Basket recommendations
	- Category activation campaigns on the Home page
- Current plan to mitigate customer impact:
	- Send push notification to customers
	- Extend slot
-
- **Decision**: we are going to slowly start enabling the following elements (with intervals of 5 minutes):
	- Previously bought ("Opnieuw bestellen")
	- Your promotions ("Jouw acties")
	- Basket recommendations
	- Category activation campaigns on the Home page
-
-
- **Update 21:55**
-
- Store was down between: 20.15-21.35 (1.5-2.0k missed orders)
-
- Current ideas to mitigate customer impact:
	- Send push notification to impacted customers.
		- :loading: Getting the IDs of the customers
		- :loading: Asking for help to Selligent operator to send the notifications.
	- Potentially: extending cut-off time, to 23:30 or 24:00.
-
- Currently slowly re-enabling features, first one showed no significant impact, currently enabled:
	- Previously bought
	- Your promotions
-
- **Update 22:09**
- Small update on contacting users
	- Around 23k customers tried to log in, we now have the IDs.
	- [Thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711573654191089) for making a decision on whether we should send a push
-
- Re-enabling elements:
	- :check: Basket recommendations now also enabled.
	- :loading: Category activation is left.
-
-
- **Update 22:18**
- **Business decision**: we will not push to customers, [thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711574169750729?thread_ts=1711573654.191089&cid=C06S1EAEVR7).
-
-
- **Update 22:28**
- @Ihab mentioned nothing malicious, no scraper or weird behavior, everything seems to be distributed.
-
- There is a chance we are reconsidering to extend cut-off, see [thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711574636559649?thread_ts=1711573954.502889&cid=C06S1EAEVR7).
-
- Update 22:35
- On re-enabling components; we're waiting with enabling category activation till tomorrow ([thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711574974021579?thread_ts=1711572017.790569&cid=C06S1EAEVR7)).
-
- Decision: will not extend cut-off time, [thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711575140504929?thread_ts=1711573954.502889&cid=C06S1EAEVR7).
-
- **Update 22:40:**
- We will scale down horizontally to limit costs.
	- To minimize risk, we wait till 23.00 with downscaling. There is still quite some requests coming in, more than yesterday.
-
- Root cause still unclear, investigation will continue tomorrow.
-
- **Update 23:00**
- Currently still at x3 the traffic compared to yesterday. Once this get's to a normal state, **we'll scale down** from 30 -> 24 pods.
- After some time (depending on traffic and resource utilization) we scale down to 24 -> 16.
- No scaling down vertically today, to minimize risk.
-
- Root cause still **unclear**.
	- Current thinking on the problem; we were not able to fulfill the requests. As a result, the queue of calcite requests to process piled up, up to 100k. Consequently, we were not able to get to processing again and everything was failing.
	- **Observation**: based on the number of requests per user we can rule out a malicious actor (max number of requests for the "top" user was 160-170).
	- **Observation**: possibly this behavior possibly started on the 21st already, [graph](https://grafana-prod.nl.picnicinternational.com/d/i1U1ZL3nk/ppp-overview?orgId=1&from=1710111600000&to=1711666799000&viewPanel=3).  Store was released, some categories were activated.
-
- **Updating 23:15**
- Around 23:10 we scaled down from 30 -> 24 pods.
- Soon we will scale down to 16 pods.
-
- Tomorrow we'll have to continue the investigation, most likely start with the release of the 20st ([thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711577485983759?thread_ts=1711576858.662659&cid=C06S1EAEVR7)). Current thinking something got released and over the last week it slowly but surely had more impact on the Store.
-
- **Observation**: Autoscaling during peak hours went to 30 pods (which is the maximum) already for three days, so we were building up to this point.
-
- Update 23:30
- This will be the last update for tonight.
-
- We are back at 16 pods.
- Auto-scaling is still enabled, tomorrow morning's traffic is not worrying us.
-
- **Consider tomorrow morning**: think about whether to scale down vertically in the morning ([thread](https://teampicnic.slack.com/archives/C06S1EAEVR7/p1711578117679049?thread_ts=1711577590.706949&cid=C06S1EAEVR7))
-